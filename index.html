<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ViT-VS: Generalizable Visual Servoing with Vision Transformers</title>
  
  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  
  <!-- Bulma CSS Framework -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  
  <!-- Custom CSS -->
  <link rel="stylesheet" href="static/css/style.css">
</head>
<body>

  <!-- Hero Section with Title -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ViT-VS</h1>
            <h2 class="title is-3 publication-title">On the Applicability of Pretrained Vision Transformer Features for Generalizable Visual Servoing</h2>
            
            <!-- Authors -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <span style="color: #0000FF;">Alessandro Scherl</span><sup>1,2</sup>
              </span>
              <span class="author-block">
                <span style="color: #0000FF;">Stefan Thalhammer</span><sup>2</sup>
              </span>
              <span class="author-block">
                <span style="color: #0000FF;">Bernhard Neuberger</span><sup>2</sup>
              </span>
              <span class="author-block">
                <span style="color: #0000FF;">Wilfried Wöber</span><sup>2</sup>
              </span>
              <span class="author-block">
                <span style="color: #0000FF;">José García-Rodríguez</span><sup>1</sup>
              </span>
            </div>

            <!-- Affiliations -->
            <div class="is-size-6 publication-affiliations">
              <span class="affiliation-block"><sup>1</sup>Department of Computer Technology, University of Alicante, Spain</span>
              <br>
              <span class="affiliation-block"><sup>2</sup>Industrial Engineering Department, UAS Technikum Vienna, Austria</span>
            </div>

            <!-- Links/Buttons -->
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Paper Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.04545" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                
                <!-- Code Link -->
                <span class="link-block">
                  <a href="https://github.com/AlessandroScherl/ViT-VS" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                
                <!-- Citation -->
                <span class="link-block">
                  <a href="#" id="cite-button" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-quote-right"></i>
                    </span>
                    <span>Cite</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser Image Section -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <figure class="image">
          <img src="static/images/teaser.jpg" alt="ViT-VS Teaser">
          <figcaption class="has-text-centered mt-2">
            ViT-VS category-level object grasping. Left: ViT Correspondence Matching process with white mug as desired image. Right: Successful grasp of the target object.
          </figcaption>
        </figure>
      </div>
    </div>
  </section>

  <!-- Abstract Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Visual servoing enables robots to precisely position their end-effector relative to a target object. While classical methods rely on hand-crafted features and thus are universally applicable without task-specific training, they often struggle with occlusions and environmental variations, whereas learning-based approaches improve robustness but typically require extensive training. We present a visual servoing approach that leverages pretrained vision transformers for semantic feature extraction, combining the advantages of both paradigms while also being able to generalize beyond the provided sample. Our approach achieves full convergence in unperturbed scenarios and surpasses classical image-based visual servoing by up to 31.2% relative improvement in perturbed scenarios. Even the convergence rates of learning-based methods are matched despite requiring no task- or object-specific training. Real-world evaluations confirm robust performance in end-effector positioning, industrial box manipulation, and grasping of unseen objects using only a reference from the same category.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method Overview -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method Overview</h2>
          <div class="content has-text-justified">
            <div class="video-container has-text-centered">
              <video controls autoplay loop muted class="has-ratio" width="100%">
                <source src="static/videos/intro.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <p class="mt-4">
              Our ViT-VS approach combines Vision Transformer (ViT) correspondence matching with classical image-based visual servoing (IBVS) principles. The methodology addresses key challenges of using pretrained ViTs for robotic control, including rotation invariance compensation and velocity stabilization for smoother trajectories.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Experiments Section -->
  <section class="section" id="experiments">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experiments</h2>
      
      <!-- Tabs for different experiments -->
      <div class="tabs is-centered is-boxed experiment-tabs">
        <ul>
          <li data-target="grasping-tab" class="is-active">
            <a>
              <span class="icon is-small"><i class="fas fa-hand-rock"></i></span>
              <span>Category-Level Grasping</span>
            </a>
          </li>
          <li data-target="sorting-tab">
            <a>
              <span class="icon is-small"><i class="fas fa-sort"></i></span>
              <span>Category-Level Sorting</span>
            </a>
          </li>
          <li data-target="industrial-tab">
            <a>
              <span class="icon is-small"><i class="fas fa-industry"></i></span>
              <span>Industrial Use-Case</span>
            </a>
          </li>
          <li data-target="positioning-tab">
            <a>
              <span class="icon is-small"><i class="fas fa-robot"></i></span>
              <span>Detailed Experiment</span>
            </a>
          </li>
          <li data-target="simulation-tab">
            <a>
              <span class="icon is-small"><i class="fas fa-vr-cardboard"></i></span>
              <span>Simulation</span>
            </a>
          </li>
        </ul>
      </div>
      
      <!-- Tab content for different experiments -->
      <div class="tab-contents">
        <!-- Category-Level Grasping -->
        <div id="grasping-tab" class="tab-content is-active">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-4">Category-Level Object Grasping</h3>
              <div class="video-container has-text-centered">
                <video controls autoplay loop muted class="has-ratio" width="100%">
                  <source src="static/videos/grasping.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <div class="content mt-4">
                <p>
                  Our method demonstrates robust performance across all object categories, achieving success rates of 100% for shoes, 90% for mugs, and 80% for toy cars. This experiment shows a representative grasping sequence of an unseen blue toy car.
                </p>
                <table class="table is-fullwidth">
                  <thead>
                    <tr>
                      <th>Object Category</th>
                      <th>Success Rate</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Shoe</td>
                      <td>10/10 (100%)</td>
                    </tr>
                    <tr>
                      <td>Mug</td>
                      <td>9/10 (90%)</td>
                    </tr>
                    <tr>
                      <td>Toy Car</td>
                      <td>8/10 (80%)</td>
                    </tr>
                    <tr>
                      <td><strong>Overall</strong></td>
                      <td><strong>27/30 (90%)</strong></td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Category-Level Sorting -->
        <div id="sorting-tab" class="tab-content">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-4">Category-Level Object Sorting</h3>
              <div class="video-container has-text-centered">
                <video controls autoplay loop muted class="has-ratio" width="100%">
                  <source src="static/videos/sorting.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <div class="content mt-4">
                <p>
                  Building on our category-level grasping capabilities, we demonstrate a complete pick-and-place sorting task using only reference images from each category. The system identifies unseen objects from the same categories and successfully sorts them into designated locations with a high success rate.
                </p>
                <p>
                  This experiment highlights the practical application of our ViT-VS approach in a realistic object manipulation scenario, demonstrating how pretrained vision transformers can enable generalized robotic manipulation without task-specific training.
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Industrial Use-Case -->
        <div id="industrial-tab" class="tab-content">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-4">Industrial Box Manipulation</h3>
              <div class="video-container has-text-centered">
                <video controls autoplay loop muted class="has-ratio" width="100%">
                  <source src="static/videos/industrial.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <div class="content mt-4">
                <p>
                  We demonstrate industrial box manipulation with 100% success rate (n = 20) on a mobile robot with a starting point positioning error of ±10cm. This experiment shows the robustness of our approach in real-world industrial scenarios.
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Detailed Experiment -->
        <div id="positioning-tab" class="tab-content">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-4">Detailed Experiment</h3>
              <div class="video-container has-text-centered">
                <video controls autoplay loop muted class="has-ratio" width="100%">
                  <source src="static/videos/positioning.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <div class="content mt-4">
                <p>
                  This experiment demonstrates real-world evaluation on the "hollywood poster" for 1500 iterations. A detailed visualization and analysis of initial image, desired image, final image, camera velocities and position errors is presented in our paper, demonstrating full convergence from a partially visible and heavily rotated initial position.
                </p>
              </div>
            </div>
          </div>
        </div>
        
        <!-- Simulation Experiments -->
        <div id="simulation-tab" class="tab-content">
          <div class="columns is-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-4">Simulation Experiments</h3>
              <div class="video-container has-text-centered">
                <video controls autoplay loop muted class="has-ratio" width="100%">
                  <source src="static/videos/simulation.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <div class="content mt-4">
                <p>
                  Our approach achieves full convergence in unperturbed scenarios and surpasses classical image-based visual servoing by up to 31.2% relative improvement in perturbed scenarios. Even the convergence rates of learning-based methods are matched despite requiring no task- or object-specific training.
                </p>
                <table class="table is-fullwidth">
                  <thead>
                    <tr>
                      <th>Method</th>
                      <th>Convergence Rate (Unperturbed/Perturbed)</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>ViT-VS (ours)</td>
                      <td>100.0% / 76.6%</td>
                    </tr>
                    <tr>
                      <td>Classical IBVS</td>
                      <td>89.6% / 58.4%</td>
                    </tr>
                    <tr>
                      <td>Deep Learning-based</td>
                      <td>100.0% / 76.0%</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Citation Modal -->
  <div id="citation-modal" class="modal">
    <div class="modal-background"></div>
    <div class="modal-card">
      <header class="modal-card-head">
        <p class="modal-card-title">Citation</p>
        <button class="delete" aria-label="close"></button>
      </header>
      <section class="modal-card-body">
        <div class="content">
          <pre id="bibtex-citation">@article{scherl2025vit-vs,
  title={ViT-VS: On the Applicability of Pretrained Vision Transformer Features for Generalizable Visual Servoing},
  author={Scherl, Alessandro and Thalhammer, Stefan and Neuberger, Bernhard and W{\"o}ber, Wilfried and Garc{\'i}a-Rodr{\'i}guez, Jos{\'e}},
  journal={arXiv preprint arXiv:2503.04545},
  year={2025}
}</pre>
        </div>
        <button class="button is-info" id="copy-citation">
          <span class="icon">
            <i class="fas fa-copy"></i>
          </span>
          <span>Copy to Clipboard</span>
        </button>
      </section>
    </div>
  </div>

  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          ViT-VS: On the Applicability of Pretrained Vision Transformer Features for Generalizable Visual Servoing
        </p>
        <p>
          <a href="https://alessandroscherl.github.io/ViT-VS/">https://alessandroscherl.github.io/ViT-VS/</a>
        </p>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script src="static/js/main.js"></script>
</body>
</html>
